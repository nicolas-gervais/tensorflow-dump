import tensorflow as tf
import tensorflow_datasets as tfds

train, test = tfds.load('mnist',
                        split=['train[:80%]', 'train[80%:]'],
                        as_supervised=True)

def preprocess(x, y):
    x = tf.divide(x, 255)
    x = tf.image.resize_with_crop_or_pad(x, 28, 28)
    return x, y

AUTOTUNE = tf.data.experimental.AUTOTUNE

train_ds = train.map(preprocess).\
    shuffle(128).\
    batch(8, drop_remainder=True).\
    prefetch(AUTOTUNE)
test_ds = test.map(preprocess).\
    shuffle(128).\
    batch(8, drop_remainder=True).\
    prefetch(AUTOTUNE)


class CustomDenseLayer(tf.Module):
  def __init__(self, in_features, out_features, activation, name=None):
    super(CustomDenseLayer, self).__init__(name=name)
    self.activation = activation
    self.w = tf.Variable(
      tf.initializers.GlorotUniform()([in_features, out_features]), name='weights',
    trainable=True)
    self.b = tf.Variable(tf.zeros([out_features]), name='biases',
                         trainable=True)
  def __call__(self, x):
    y = tf.matmul(x, self.w) + self.b
    return self.activation(y)


class CustomConvLayer(tf.Module):
    def __init__(self, in_channels, filters, kernel_size, padding, strides, activation,
                 kernel_initializer, bias_initializer, use_bias):
        super(CustomConvLayer, self).__init__()
        self.filters = filters
        self.kernel_size = kernel_size
        self.activation = activation
        self.padding = padding
        self.kernel_initializer = kernel_initializer
        self.bias_initializer = bias_initializer
        self.strides = strides
        self.use_bias = use_bias
        self.in_channels = in_channels

        self.w = tf.Variable(
            initial_value=self.kernel_initializer(shape=(*self.kernel_size,
                                                         in_channels,
                                                         self.filters),
                                 dtype='float32'), trainable=True)
        if self.use_bias:
            self.b = tf.Variable(
                initial_value=self.bias_initializer(shape=(self.filters,),
                                                    dtype='float32'),
                trainable=True)

    def __call__(self, inputs, training=None):
        x =  tf.nn.conv2d(inputs, filters=self.w, strides=self.strides,
                          padding=self.padding)
        if self.use_bias:
            x = x + self.b
        x = self.activation(x)
        return x


class ModelWithCustomLayers(tf.Module):
    def __init__(self):
        super(ModelWithCustomLayers, self).__init__()
        self.conv1 = CustomConvLayer(
            in_channels=1,
            filters=16,
            kernel_size=(3, 3),
            strides=(1, 1),
            activation=tf.nn.relu,
            padding='VALID',
            kernel_initializer=tf.initializers.GlorotUniform(seed=42),
            bias_initializer=tf.initializers.Zeros(),
            use_bias=True)
        self.conv2 = CustomConvLayer(
            in_channels=16,
            filters=32,
            kernel_size=(3, 3),
            strides=(1, 1),
            activation=tf.nn.relu,
            padding='VALID',
            kernel_initializer=tf.initializers.GlorotUniform(seed=42),
            bias_initializer=tf.initializers.Zeros(),
            use_bias=True)
        self.dense1 = CustomDenseLayer(
            in_features=32,
            out_features=32,
            activation=tf.nn.relu)
        self.dense2 = CustomDenseLayer(
            in_features=32,
            out_features=10,
            activation=tf.nn.softmax)

    def __call__(self, inputs):
        x = self.conv1(inputs)
        x = tf.nn.max_pool2d(x, ksize=2, strides=1, padding='VALID')
        x = self.conv2(x)
        x = tf.nn.max_pool2d(x, ksize=2, strides=1, padding='VALID')
        x = tf.reduce_mean(x, axis=(1, 2))
        x = self.dense1(x)
        x = self.dense2(x)
        return x


custom = ModelWithCustomLayers()

loss_object = tf.losses.sparse_categorical_crossentropy

def compute_loss(model, x, y):
    out = model(x)
    loss = loss_object(y, out, from_logits=False)
    return out, loss

def compute_gradients(model, x, y):
    with tf.GradientTape() as tape:
        out, loss_val = compute_loss(model, x, y)
    grads = tape.gradient(loss_val, model.trainable_variables)
    return out, loss_val, grads

optimizer = tf.optimizers.Adam(lr=0.01)
batch_size = train_ds._input_dataset._batch_size.numpy()

def sparse_cat_acc(y_true, y_pred):
    argmax_out = tf.argmax(y_pred, axis=-1)
    equals = tf.equal(argmax_out, y_true)
    cast = tf.cast(equals, tf.float32)
    mean = tf.reduce_mean(cast)
    return mean


for epoch in range(1, 10 + 1):
    train_loss = tf.zeros(shape=())
    test_loss = tf.zeros(shape=())

    train_acc = tf.zeros(shape=())
    test_acc = tf.zeros(shape=())


    for train_ix, (input_batch, label_batch) in enumerate(train_ds, 1):
        out, loss_val, grads = compute_gradients(custom, input_batch, label_batch)
        optimizer.apply_gradients(zip(grads, custom.trainable_variables))

        train_loss += tf.reduce_sum(loss_val)
        train_acc += sparse_cat_acc(label_batch, out)

    for test_ix, (input_batch, label_batch) in enumerate(test_ds, 1):
        out, loss_val = compute_loss(custom, input_batch, label_batch)

        test_loss += tf.reduce_sum(loss_val)
        test_acc += sparse_cat_acc(label_batch, out)

    print(f'Epoch {epoch:2d} '
          f'Loss {train_loss/(train_ix*batch_size):=5.3f} '
          f'Acc {train_acc/train_ix:=5.3f} '
          f'TLoss {test_loss/(test_ix*batch_size):=5.3f} '
          f'TAcc {test_acc/test_ix:=5.3f}')


