import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.chdir('dogsvscats')
from tensorflow import keras as K
import tensorflow
from collections import deque


train_generator = K.preprocessing.image.ImageDataGenerator(
    rotation_range=30,
    width_shift_range=1.,
    height_shift_range=1.,
    brightness_range=(5e-1, 15e-1),
    zoom_range=(0.7, 1.3),
    horizontal_flip=True,
    rescale=1/255
)

test_generator = K.preprocessing.image.ImageDataGenerator(
    rescale=1/255
)

BATCH_SIZE = 8

train_iterator = train_generator.flow_from_directory(
    directory='training_set',
    target_size=(150, 150),
    batch_size=BATCH_SIZE,
    class_mode='binary'
)

test_iterator = test_generator.flow_from_directory(
    directory='test_set',
    target_size=(150, 150),
    batch_size=BATCH_SIZE,
    class_mode='binary'
)


base = K.applications.Xception(
    include_top=False,
    input_tensor=None,
    input_shape=(150, 150, 3),
    classes=2,
    pooling='max'
)

# model = K.layers.Dense(2)

# CNN = K.Model(inputs=base.input, outputs=model(base.output))
CNN = base

for layer in base.layers:
    layer.trainable = True

loss_object = K.losses.SparseCategoricalCrossentropy(from_logits=True)

train_loss = K.metrics.Mean()
test_loss = K.metrics.Mean()

train_acc = K.metrics.SparseCategoricalAccuracy()
test_acc = K.metrics.SparseCategoricalAccuracy()

optimizer = K.optimizers.Adam(learning_rate=1e-3)


@tensorflow.function
def train_step(inputs, labels):
    with tensorflow.GradientTape() as tape:
        logits = CNN(inputs, training=True)
        loss = loss_object(labels, logits)

    gradients = tape.gradient(loss, CNN.trainable_variables)
    optimizer.apply_gradients(zip(gradients, CNN.trainable_variables))

    train_loss(loss)
    train_acc(labels, logits)


@tensorflow.function
def test_step(inputs, labels):
    logits = CNN(inputs, training=False)
    loss = loss_object(labels, logits)
    test_loss(loss)
    test_acc(labels, logits)


def test_and_learn(epochs=1, early_stop=10):
    template = 'Iteration {:2} Loss {:6.4f} Val Loss {:6.4f} Acc {:6.2%} Val Acc {:6.2%}'
    loss_history = deque(maxlen=early_stop + 1)

    for epoch in range(1, epochs + 1):
        train_loss.reset_states()
        test_loss.reset_states()
        train_acc.reset_states()
        test_acc.reset_states()

        for ix, (inputs, labels) in enumerate(train_iterator):
            train_step(inputs, labels)
            if ix >= (8000//BATCH_SIZE):
                break


        for ix, (inputs, labels) in enumerate(test_iterator):
            test_step(inputs, labels)
            if ix >= (2000//BATCH_SIZE):
                break

        print(template.format(
            epoch,
            train_loss.result(),
            test_loss.result(),
            train_acc.result(),
            test_acc.result()
        ))

        loss_history.append(test_loss.result())

        if len(loss_history) > early_stop and loss_history.popleft() < min(loss_history):
            print('Early stopping. No validation loss decrease in %i epochs.' % early_stop)
            break


if __name__ == '__main__':
    test_and_learn(epochs=50, early_stop=10)


------------------------------------------------------------------
Iteration  25 Loss 0.0727 Val Loss 0.1372 Acc 97.14% Val Acc 95.02%
Iteration  26 Loss 0.0805 Val Loss 0.0934 Acc 96.79% Val Acc 96.31%
Iteration  27 Loss 0.0679 Val Loss 0.1082 Acc 97.49% Val Acc 96.41%
Iteration  28 Loss 0.0652 Val Loss 0.1087 Acc 97.56% Val Acc 96.71%
Iteration  29 Loss 0.0694 Val Loss 0.0964 Acc 97.47% Val Acc 96.61%
Iteration  30 Loss 0.0685 Val Loss 0.1112 Acc 97.58% Val Acc 95.82%
Iteration  31 Loss 0.0669 Val Loss 0.1251 Acc 97.38% Val Acc 95.97%
Iteration  32 Loss 0.0605 Val Loss 0.1380 Acc 97.74% Val Acc 94.87%
-------------------------------------------------------------------
