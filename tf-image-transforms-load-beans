import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
import tensorflow as tf
from tensorflow import keras
import tensorflow_datasets as tfds
from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, GaussianDropout, Flatten
from tensorflow.keras.models import Model
from tensorflow.keras.regularizers import l1, l2, l1_l2
import tensorflow_transform as tft

(train_data, test_data), info = tfds.load(name='beans', split=['train', 'validation'], with_info=True)
data = tfds.as_numpy(train_data)


def train_processing(tensor):
    tensor = tf.cast(x=tensor, dtype=tf.float32)
    tensor = tf.image.rgb_to_grayscale(images=tensor)
    tensor = tf.image.resize(images=tensor, size=(96, 96), method=tf.image.ResizeMethod.GAUSSIAN)
    tensor = tf.divide(x=tensor, y=tf.constant(255.))
    tensor = tf.image.random_flip_left_right(image=tensor)
    tensor = tf.image.random_brightness(image=tensor, max_delta=2e-1)
    tensor = tf.image.random_crop(value=tensor, size=(48, 48, 1))
    return tensor


def test_processing(tensor):
    tensor = tf.cast(x=tensor, dtype=tf.float32)
    tensor = tf.image.rgb_to_grayscale(images=tensor)
    tensor = tf.image.resize(images=tensor, size=(96, 96), method=tf.image.ResizeMethod.GAUSSIAN)
    tensor = tf.divide(x=tensor, y=tf.constant(255.))
    tensor = tf.image.central_crop(image=tensor, central_fraction=5e-1)
    return tensor


train = train_data.shuffle(info.splits['train'].num_examples).repeat(5)
test = train_data.shuffle(info.splits['validation'].num_examples)

train_set = train.map(lambda x: (train_processing(x['image']), x['label'])).batch(16)
test_set  = test. map(lambda x: (test_processing( x['image']), x['label'])).batch(16)


# for _, labels in train_set.take(1):  # tfds.as_numpy(train_set.take(1))
#     print(labels)
# 
# for images, _ in train_set.take(1):
#     print(images.shape)

info.features['image'].shape
info.features['label'].num_classes
info.splits['train'].num_examples
info.splits['validation'].num_examples

next(iter(train_data))['image']


class ConvNet(Model):
    def __init__(self):
        super(ConvNet, self).__init__()
        self.conv1 = Conv2D(filters=8,
                            kernel_size=(3, 3),
                            activation='relu',
                            kernel_regularizer=l1(l=1e-2))
        self.maxp1 = MaxPool2D(pool_size=(2, 2))
        self.conv2 = Conv2D(filters=16,
                            kernel_size=(3, 3),
                            activation='relu',
                            kernel_regularizer=l2(l=1e-2))
        self.maxp2 = MaxPool2D(pool_size=(2, 2))
        self.conv3 = Conv2D(filters=32,
                            kernel_size=(3, 3),
                            activation='relu',
                            kernel_regularizer=l1_l2(l1=15e-3, l2=5e-3))
        self.maxp3 = MaxPool2D(pool_size=(2, 2))
        self.flat = Flatten()
        self.dense1 = Dense(units=32, activation='relu')
        self.dense2 = Dense(units=info.features['label'].num_classes)

    def call(self, inputs, training=None, mask=None):
        x = self.conv1(inputs)
        x = self.maxp1(x)
        x = self.conv2(x)
        x = self.maxp2(x)
        x = self.conv3(x)
        x = self.maxp3(x)
        x = self.flat(x)
        x = self.dense1(x)
        x = self.dense2(x)
        return x


cnn = ConvNet()

cnn(next(iter(train_set))[0])

loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

train_loss = tf.keras.metrics.Mean()
test_loss = tf.keras.metrics.Mean()

train_acc = tf.keras.metrics.SparseCategoricalAccuracy()
test_acc = tf.keras.metrics.SparseCategoricalAccuracy()

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)


@tf.function
def train_step(inputs, labels):
    with tf.GradientTape() as tape:
        logits = cnn(inputs, training=True)
        loss = loss_object(labels, logits)

    gradients = tape.gradient(loss, cnn.trainable_variables)
    optimizer.apply_gradients(zip(gradients, cnn.trainable_variables))

    train_loss(loss)
    train_acc(labels, logits)


@tf.function
def test_step(inputs, labels):
    logits = cnn(inputs, training=False)
    loss = loss_object(labels, logits)
    test_loss(loss)
    test_acc(labels, logits)


def learn(epochs=5):
    template = 'epoch {:0>3} train_loss {:>5.3f} test_loss {:>5.3f} ' \
               'train_acc {:>5.3f} test_acc {:>5.3f}'
    train_loss.reset_states()
    train_acc.reset_states()
    test_loss.reset_states()
    test_acc.reset_states()
    
    for epoch in range(1, epochs + 1):
        for images, labels in train_set:
            train_step(images, labels)
    
        for images, labels in test_set:
            test_step(images, labels)
    
        print(template.format(
              epoch,
              train_loss.result(),
              test_loss.result(),
              train_acc.result(),
              test_acc.result()))


if __name__ == '__main__':
    learn(epochs=100)
    
    
-------------------------------------------------------------------------
epoch 091 train_loss 0.762 test_loss 0.688 train_acc 0.653 test_acc 0.703
epoch 092 train_loss 0.761 test_loss 0.685 train_acc 0.654 test_acc 0.704
epoch 093 train_loss 0.760 test_loss 0.684 train_acc 0.655 test_acc 0.705
epoch 094 train_loss 0.758 test_loss 0.682 train_acc 0.656 test_acc 0.706
epoch 095 train_loss 0.757 test_loss 0.680 train_acc 0.657 test_acc 0.707
epoch 096 train_loss 0.755 test_loss 0.678 train_acc 0.658 test_acc 0.708
epoch 097 train_loss 0.754 test_loss 0.676 train_acc 0.659 test_acc 0.709
epoch 098 train_loss 0.753 test_loss 0.675 train_acc 0.660 test_acc 0.710
epoch 099 train_loss 0.751 test_loss 0.673 train_acc 0.660 test_acc 0.711
epoch 100 train_loss 0.750 test_loss 0.671 train_acc 0.661 test_acc 0.712
-------------------------------------------------------------------------
