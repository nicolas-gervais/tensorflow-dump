import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
from tensorflow import keras as K
import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras.layers import Conv2D, MaxPool2D, Conv2DTranspose, Reshape
import tensorflow_datasets as tfds
from functools import partial
from collections import deque

mnist = tfds.load('mnist')

train, test = mnist['train'], mnist['test']

rescale = lambda x: tf.divide(x['image'], 255)

ds = train.concatenate(test).map(rescale).batch(8)


class ConvolutionalEncoder(Model):
    """Convolutional Encoder"""
    def __init__(self):
        super(ConvolutionalEncoder, self).__init__()

        custom_convolution = partial(Conv2D, kernel_size=(3, 3),
                                     strides=(1, 1),
                                     activation='relu',
                                     padding='same')
        custom_pooling = partial(MaxPool2D, pool_size=(2, 2))

        self.conv1 = custom_convolution(filters=16, input_shape=(28, 28, 1))
        self.maxp1 = custom_pooling()
        self.conv2 = custom_convolution(filters=32)
        self.maxp2 = custom_pooling()
        self.conv3 = custom_convolution(filters=64)
        self.maxp3 = custom_pooling()

    def call(self, x, training=None, **kwargs):
        x = self.conv1(x)
        x = self.maxp1(x)
        x = self.conv2(x)
        x = self.maxp2(x)
        x = self.conv3(x)
        x = self.maxp3(x)
        return x


class ConvolutionalDecoder(Model):
    """Convolutional Decoder"""
    def __init__(self):
        super(ConvolutionalDecoder, self).__init__()
        custom_transpose = partial(Conv2DTranspose,
                                   padding='same',
                                   kernel_size=(3, 3),
                                   activation='relu',
                                   strides=(2, 2))

        self.trans1 = custom_transpose(filters=32, input_shape=(3, 3, 32), padding='valid')
        self.trans2 = custom_transpose(filters=16)
        self.trans3 = custom_transpose(filters=1, activation='sigmoid')
        self.reshape1 = Reshape(target_shape=[28, 28, 1])

    def call(self, x, training=None, **kwargs):
        x = self.trans1(x)
        x = self.trans2(x)
        x = self.trans3(x)
        x = self.reshape1(x)
        return x


class ConvolutionalAutoEncoder(Model):
    """Convolutional Auto-Encoder"""
    def __init__(self):
        super(ConvolutionalAutoEncoder, self).__init__()
        self.encoder = ConvolutionalEncoder()
        self.decoder = ConvolutionalDecoder()

    def call(self, x, training=None, **kwargs):
        x = self.encoder(x)
        x = self.decoder(x)
        return x


autoencoder = ConvolutionalAutoEncoder()

loss_fn = K.losses.BinaryCrossentropy(from_logits=False)
reconstruction_loss = K.metrics.Mean(name='loss')
optimizer = K.optimizers.Adam(learning_rate=1e-3)


@tf.function
def reconstruct(inputs):
    with tf.GradientTape() as tape:
        out = autoencoder(inputs)
        loss = loss_fn(inputs, out)

    gradients = tape.gradient(loss, autoencoder.trainable_variables)
    optimizer.apply_gradients(zip(gradients, autoencoder.trainable_variables))

    reconstruction_loss.update_state(loss)


def iterate_and_learn(epochs=10, early_stop=None):
    template = 'Epoch {:2} Reconstruction Loss {:6.4f}'
    if early_stop:
        loss_history = deque(maxlen=epochs+1)

    for epoch in range(epochs):
        reconstruction_loss.reset_states()
        for samples in ds:
            reconstruct(samples)

        print(template.format(epoch + 1, reconstruction_loss.result()))
        if early_stop:
            loss_history.append(reconstruction_loss.result())
            if epoch > early_stop and loss_history.popleft() < min(loss_history):
                print('Early stopping. No loss decrease in %i epochs.' % early_stop)
                break


if __name__ == '__main__':
    iterate_and_learn(epochs=25, early_stop=5)

-----------------------------------
Epoch 20 Reconstruction Loss 0.0663
Epoch 21 Reconstruction Loss 0.0662
Epoch 22 Reconstruction Loss 0.0662
Epoch 23 Reconstruction Loss 0.0661
Epoch 24 Reconstruction Loss 0.0661
Epoch 25 Reconstruction Loss 0.0660
-----------------------------------
