import tensorflow as tf
from tensorflow import keras
import numpy as np
from tensorflow.keras import Model, optimizers

(xtrain, ytrain), (xtest, ytest) = keras.datasets.mnist.load_data()

xtrain = np.float32(xtrain/255)
xtest = np.float32(xtest/255)

ytrain = np.int32(ytrain)
ytest = np.int32(ytest)

def pre_process(inputs, targets):
    inputs = tf.expand_dims(inputs, -1)
    targets = tf.one_hot(targets, depth=10)
    return tf.divide(inputs, 255), targets

train_data = tf.data.Dataset.from_tensor_slices((xtrain, ytrain)).take(10_000).shuffle(10_000).batch(8).map(pre_process)
test_data = tf.data.Dataset.from_tensor_slices((xtest, ytest)).take(1_000).shuffle(1_000).batch(8).map(pre_process)

from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Dropout, Flatten

class CNN(Model):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = Conv2D(filters=16, kernel_size=(3, 3), strides=(1, 1), input_shape=(28, 28, 1))
        self.maxp1 = MaxPool2D(pool_size=(2, 2))
        self.conv2 = Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1))
        self.maxp2 = MaxPool2D(pool_size=(2, 2))
        self.flat1 = Flatten()
        self.dens1 = Dense(64, activation='relu')
        self.drop1 = Dropout(5e-1)
        self.dens3 = Dense(10)

    def call(self, x):
        x = self.conv1(x)
        x = self.maxp1(x)
        x = self.conv2(x)
        x = self.maxp2(x)
        x = self.flat1(x)
        x = self.dens1(x)
        x = self.drop1(x)
        x = self.dens3(x)
        return x

model = CNN()

opt = optimizers.Adam(learning_rate=1e-3)
compute_loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)
compute_acc = tf.keras.metrics.CategoricalAccuracy()

def train_step(model, optimizer, images, labels):
    with tf.GradientTape() as tape:
        logits = model(images)
        loss = compute_loss(labels, logits)
        compute_acc(labels, logits)

    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))

    return loss

def train(model, optimizer, dataset, epoch, log_freq=50):
    mean_loss = tf.keras.metrics.Mean(dtype=tf.float32)

    for ix, (images, labels) in enumerate(dataset, 1):
        loss = train_step(model=model, optimizer=optimizer, images=images, labels=labels)
        mean_loss(loss)

        if tf.equal(optimizer.iterations % log_freq, 0):
            print(f'EPOCH {epoch:>2}  STEP {ix*8:>4}/10,000  LOSS {(mean_loss.result().numpy()):.6f}'
                  f'  ACC {compute_acc.result().numpy():.2%}')
            mean_loss.reset_states()
            compute_acc.reset_states()

def test_step(model, dataset):
    mean_loss = tf.keras.metrics.Mean(dtype=tf.float32)

    for images, labels in dataset:

        logits = model(images)
        mean_loss(compute_loss(labels, logits))
        compute_acc(labels, logits)

    print(f'<TEST> {" "*20} LOSS {(mean_loss.result().numpy()):.6f}'
          f'  ACC {compute_acc.result().numpy():.2%}')


for i in range(1, 10 + 1):
    train(model=model, optimizer=opt, dataset=train_data, log_freq=5_0, epoch=i)
    test_step(model, test_data)
    
-------------------------------------------------------
EPOCH  1  STEP  7,200/10,000  LOSS 0.348091  ACC 90.25%
EPOCH  1  STEP  7,600/10,000  LOSS 0.289117  ACC 92.00%
EPOCH  1  STEP  8,000/10,000  LOSS 0.298139  ACC 90.00%
EPOCH  1  STEP  8,400/10,000  LOSS 0.348562  ACC 89.75%
EPOCH  1  STEP  8,800/10,000  LOSS 0.303746  ACC 90.50%
EPOCH  1  STEP  9,200/10,000  LOSS 0.294724  ACC 91.25%
EPOCH  1  STEP  9,600/10,000  LOSS 0.389580  ACC 88.75%
EPOCH  1  STEP 10,000/10,000  LOSS 0.248289  ACC 92.00%
<TEST>                        LOSS 0.314315  ACC 89.80%
-------------------------------------------------------
