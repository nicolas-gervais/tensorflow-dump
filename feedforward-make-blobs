import tensorflow as tf
from tensorflow.keras import layers, optimizers, activations
from sklearn.datasets import make_blobs
from tensorflow.keras import Model

x, y = make_blobs(n_samples=2000, n_features=10, centers=7, cluster_std=15e-1)

xtrain, ytrain = x[:1000], y[:1000]
xtest, ytest = x[1000:], y[1000:]

def pre_processing(inputs, targets):
    inputs = tf.cast(inputs, tf.float32)
    targets = tf.cast(targets, tf.int64)
    return inputs, targets

def get_data(inputs, targets):
    data = tf.data.Dataset.from_tensor_slices((inputs, targets))
    data = data.map(pre_processing)
    data = data.take(count=1000).shuffle(buffer_size=1000).batch(batch_size=256)
    return data

class FeedForwardNN(Model):
    def __init__(self):
        super(FeedForwardNN, self).__init__()
        self.dens1 = layers.Dense(4, input_shape=(10,))
        self.dens2 = layers.Dense(8)
        self.dens3 = layers.Dense(16)
        self.out0 = layers.Dense(7)

    def __call__(self, x):
        x = self.dens1(x)
        x = activations.elu(x, alpha=9e-1)
        x = self.dens2(x)
        x = tf.nn.leaky_relu(x, alpha=3e-1)
        x = self.dens3(x)
        x = activations.selu(x)
        out = self.out0(x)
        return out

model = FeedForwardNN()

@tf.function
def compute_loss(logits, labels):
    return tf.reduce_mean(
        tf.nn.sparse_softmax_cross_entropy_with_logits(
            logits=logits, labels=labels))

@tf.function
def compute_accuracy(logits, labels):
    predictions = tf.argmax(logits, axis=1)
    return tf.reduce_mean(tf.cast(tf.equal(predictions, labels), tf.float32))

@tf.function
def train_step(model, optim, inputs, targets):
    with tf.GradientTape() as tape:
        logits = model(inputs)
        loss = compute_loss(logits, targets)

    grads = tape.gradient(loss, model.trainable_variables)
    optim.apply_gradients(zip(grads, model.trainable_variables))

    accuracy = compute_accuracy(logits, targets)
    return loss, accuracy

def train(epochs, model, optim, inputs, targets):
    train_data = get_data(inputs, targets)

    loss = 0.
    acc = 0.

    for step, (X_train, y_train) in enumerate(train_data):
        loss, acc = train_step(model, optim, X_train, y_train)

        if step % 500 == 0:
            print(f'EPOCH {epochs:>2}   LOSS {loss.numpy():.6f}   ACC {acc.numpy():>7.2%}')

    return loss, acc

def test(model, inputs, targets):
    test_data = get_data(inputs, targets)

    for step, (X_test, y_test) in enumerate(test_data):
        logits = model(X_test)
        loss = compute_loss(logits, y_test)
        accuracy = compute_accuracy(logits, y_test)
        if step % 500 == 0:
            print(f'<TEST>{" "*2}   LOSS {loss.numpy():.6f}   ACC {accuracy.numpy():>7.2%}')
    return loss, accuracy

optim = optimizers.Adam(learning_rate=1e-3)

for epoch in range(1, 50 + 1):
    loss, accuracy = train(epoch, model, optim, xtrain, ytrain)
    if epoch % 5 == 0:
        t_loss, t_acc = test(model, xtest, ytest)
        
--------------------------------------
EPOCH 46   LOSS 0.344365   ACC  98.44%
EPOCH 47   LOSS 0.345325   ACC  97.27%
EPOCH 48   LOSS 0.315648   ACC  97.66%
EPOCH 49   LOSS 0.305139   ACC  96.88%
EPOCH 50   LOSS 0.289025   ACC  98.83%
<TEST>     LOSS 0.275925   ACC  98.83%
--------------------------------------
